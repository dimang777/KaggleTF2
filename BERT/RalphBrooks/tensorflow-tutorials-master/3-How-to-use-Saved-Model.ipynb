{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have received a couple of requests to provide a more concrete demonstration of how to use a TensorFlow SavedModel. The following is some example code that shows how to put this together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports section is essentially the same as what was reviewed in previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (19.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "Requirement already satisfied: tensorflow-gpu==2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (2.0.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.16.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.31.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.10.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.11.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.14.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.10.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (41.5.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!yes | pip uninstall tensorflow\n",
    "! pip install tensorflow-gpu==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, glue_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLA is the optimizing compiler for machine learning\n",
    "# It can potentially increase speed by 15% with no source code changes\n",
    "USE_XLA = False\n",
    "\n",
    "# mixed precision results on https://github.com/huggingface/transformers/tree/master/examples\n",
    "# Mixed precision can help to speed up training time\n",
    "USE_AMP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.optimizer.set_jit(USE_XLA)\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although predictions can be done using a CPU, I am running this notebook using a GPU (in order to reduce the time needed to make predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# GPU USAGE\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Use an example for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorial, we looked at building a sentiment classifier for Yelp reviews. At the end of that exercise we saved our model so that we can reuse it.\n",
    "\n",
    "As seen in the command below, the SavedModel requires attention_mask, input_ids, and token_type_ids as inputs. These are the inputs that are required by the Google BERT model that we are using. Lucky for us, we can use the HuggingFace Transformers class to convert a sentence into the required inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['attention_mask'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 128)\n",
      "      name: serving_default_attention_mask:0\n",
      "  inputs['input_ids'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 128)\n",
      "      name: serving_default_input_ids:0\n",
      "  inputs['token_type_ids'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 128)\n",
      "      name: serving_default_token_type_ids:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['output_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 2)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir /home/ec2-user/SageMaker/tensorflow-tutorials/20191227 --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands are going to load our model and the tokenizer which converts words into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedmodel = tf.saved_model.load('/home/ec2-user/SageMaker/tensorflow-tutorials/20191227')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in this process is to create something that the Transformers library can process. For our example, we are going to create a dictionary with the required tensors, feed that dictionary into a data pipeline, and have the Transformers library generate input based on the pipeline. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {'idx': tf.constant(1, dtype=tf.int64), 'label': tf.constant(0, dtype=tf.int64) ,\n",
    "           'sentence': tf.constant('This is the best store that I have ever visited', dtype=tf.string)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': <tf.Tensor: id=42271, shape=(), dtype=int64, numpy=1>,\n",
       " 'label': <tf.Tensor: id=42272, shape=(), dtype=int64, numpy=0>,\n",
       " 'sentence': <tf.Tensor: id=42273, shape=(), dtype=string, numpy=b'This is the best store that I have ever visited'>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the model for prediction, you would simply replace the sentence above with the sentence that you want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensors(example)\n",
    "feature_ds = glue_convert_examples_to_features(ds, tokenizer, max_length=128, task='sst-2')\n",
    "feature_dataset = feature_ds.batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have features in the format required by the Google BERT model. The following function is going to convert these features into an actual prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dataset(feature_dataset, savedmodel):\n",
    "    \"\"\"\n",
    "    :param feature_dataset: Contains information needed for BERT\n",
    "    :param savedmodel: This is the model that has been pretrained in a sep process.\n",
    "    :return: JSON output with the predicted classification. \n",
    "    \"\"\"\n",
    "    \n",
    "    json_examples = []\n",
    "    for feature_batch in feature_dataset.take(-1):\n",
    "        feature_example = feature_batch[0]\n",
    "\n",
    "        # The SavedModel is going to generate log probabilities (logits) as to whether the sentence\n",
    "        # is negative (0) or positive (1).\n",
    "        logits = savedmodel.signatures[\"serving_default\"](attention_mask=feature_example['attention_mask'],\n",
    "                            input_ids=feature_example['input_ids'],\n",
    "                            token_type_ids=feature_example['token_type_ids'])['output_1']\n",
    "        print(f\"logits {logits}\")\n",
    "        \n",
    "        # It is more helpful to have the actual probabilities of success. The TensorFlow softmax \n",
    "        # function will convert the logits into probabilities.\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        \n",
    "        # At this point we have probabilities (probs) of whether the sentence is negative or positive. \n",
    "        # These probabilites (by definition) will always sum to 100%.\n",
    "        \n",
    "        # It would be better though if we could just report out which probability is higher. \n",
    "        # This is done with the argmax function.\n",
    "        \n",
    "        prediction = tf.math.argmax(probs, axis=1)\n",
    "\n",
    "        print(f\"probs {probs}\")\n",
    "        print(f\"prediction {prediction}\")\n",
    "\n",
    "        json_example = {\"SENTIMENT_PREDICTION\": str(prediction.numpy()[0])}\n",
    "        json_examples.append(json_example)\n",
    "\n",
    "    return json_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [[-1.6893245  1.3287338]]\n",
      "probs [[0.04661669 0.9533833 ]]\n",
      "prediction [1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'SENTIMENT_PREDICTION': '1'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_dataset(feature_dataset, savedmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above example, we have taken the sentence \"This is the best store that I have ever visited\", and used a SavedModel. \n",
    "\n",
    "The SavedModel states that there is a 95% probability that this sentence has positive sentiment. Based on that prediction, the model states that it is positive (\"1\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Before I complete the tutorial, I am going to examine a \"Negative\" sentence, and put some of the code that we just used all into one function. That code is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_example = {'idx': tf.constant(1, dtype=tf.int64), 'label': tf.constant(0, dtype=tf.int64) ,\n",
    "                    'sentence': tf.constant('This store is absolutely horrible and I hate it!!',\n",
    "                                            dtype=tf.string)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': <tf.Tensor: id=42333, shape=(), dtype=int64, numpy=1>,\n",
       " 'label': <tf.Tensor: id=42334, shape=(), dtype=int64, numpy=0>,\n",
       " 'sentence': <tf.Tensor: id=42335, shape=(), dtype=string, numpy=b'This store is absolutely horrible and I hate it!!'>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(example, tokenizer, savedmodel):\n",
    "    \"\"\"\n",
    "\n",
    "    :param example: This is a single dictionary of tensors which contains a idx, a label, and a sentence\n",
    "    :return: The prediction in JSON format. 1 is positive, and 0 is negative.\n",
    "    \"\"\"\n",
    "    # The Transformers glue_convert_examples_to_features works well with datasets. \n",
    "    # It does not work well with a dictionary of examples. \n",
    "    ds = tf.data.Dataset.from_tensors(example)\n",
    "    \n",
    "    # Use the transformers library in order to convert an English sentence into something that \n",
    "    # BERT recognizes.\n",
    "    \n",
    "    # The conversion requires giving a label (even if we don't have one). The easiest way to get around this is to get around\n",
    "    # this is to assign a default label of zero when you don't have a label. \n",
    "    \n",
    "    feature_ds = glue_convert_examples_to_features(ds, tokenizer, max_length=128, task='sst-2')\n",
    "\n",
    "    feature_dataset = feature_ds.batch(64)\n",
    "    json_examples = predict_dataset(feature_dataset, savedmodel)\n",
    "\n",
    "    return json_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [[ 1.2307757 -1.2191985]]\n",
      "probs [[0.9205595  0.07944044]]\n",
      "prediction [0]\n"
     ]
    }
   ],
   "source": [
    "json_result = predict(negative_example, tokenizer, savedmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'SENTIMENT_PREDICTION': '0'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the SavedModel successfully predicts that the sentence \"This store is absolutely horrible and I hate it!!\" is Negative (\"0\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Congratulations. You made it to the end of the tutorial and now you have a process that uses a SavedModel in order to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for the first time in 10 years, I am back on the job market looking for consulting opportunities or full time employment.  If you think I can be of help to you, feel free to reach out. I am on twitter at [@ralphbrooks](https://twitter.com/ralphbrooks) ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
